import tensorflow as tf
from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard
from data_loader import load_mnist
from vae_model import create_vae
import matplotlib.pyplot as plt
import streamlit as st
import os
import datetime
from mpl_toolkits.mplot3d import Axes3D
from gan_model import create_gan

class CustomCallbackLogger(Callback):
    """Custom callback to log learning rate and training progress"""
    def on_epoch_end(self, epoch, logs=None):
        lr = self.model.optimizer.learning_rate.numpy()
        print(f"[INFO] Epoch {epoch + 1} completed. Current Learning Rate: {lr:.6f}")
        
    def on_train_end(self, logs=None):
        print("[INFO] Training completed. Check if EarlyStopping or ReduceLROnPlateau were triggered.")
    
    def on_epoch_begin(self, epoch, logs=None):
        print(f"\n[INFO] Starting Epoch {epoch + 1}")

def get_callbacks(callbacks_list):
    """Generates callbacks based on the selected options."""
    callbacks = []

    # Add EarlyStopping callback to stop training early if validation loss stops improving
    if "EarlyStopping" in callbacks_list:
        callbacks.append(EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        ))

    # Add ModelCheckpoint callback to save the best model based on validation loss
    if "ModelCheckpoint" in callbacks_list:
        callbacks.append(ModelCheckpoint(
            filepath="best_model.keras",
            monitor="val_loss",
            save_best_only=True,
            verbose=1
        ))

    # Add ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus
    if "ReduceLROnPlateau" in callbacks_list:
        callbacks.append(ReduceLROnPlateau(
            monitor="val_loss",
            factor=0.5,
            patience=1,
            min_lr=1e-6,
            verbose=1
        ))

    # Add TensorBoard callback for detailed training visualization
    if "TensorBoard" in callbacks_list:
        log_dir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
        callbacks.append(TensorBoard(log_dir=log_dir, histogram_freq=1))

    # Add the custom logger
    callbacks.append(CustomCallbackLogger())
    return callbacks

def get_optimizer(optimizer_name, learning_rate):
    """Creates an optimizer based on the name and learning rate."""
    optimizer_name = optimizer_name.lower()
    if optimizer_name == "adam":
        return tf.keras.optimizers.Adam(learning_rate=learning_rate)
    elif optimizer_name == "sgd":
        return tf.keras.optimizers.SGD(learning_rate=learning_rate)
    elif optimizer_name == "rmsprop":
        return tf.keras.optimizers.RMSprop(learning_rate=learning_rate)
    elif optimizer_name == "adagrad":
        return tf.keras.optimizers.Adagrad(learning_rate=learning_rate)
    else:
        st.error(f"Optimizer '{optimizer_name}' not supported. Using 'adam' by default.")
        return tf.keras.optimizers.Adam(learning_rate=learning_rate)

def visualize_latent_space(encoder, x_test, num_images=1000):
    """
    Visualizes the latent space in 3D for test images.
    """
    # Extract a subset of test data
    sample_images = x_test[:num_images]
    
    # Pass the images through the encoder to get latent spaces
    z_mean, _, _ = encoder.predict(sample_images)
    
    # Ensure that latent_dim >= 3 for 3D visualization
    if z_mean.shape[1] >= 3:
        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')
        
        # Use only the first 3 dimensions for visualization
        ax.scatter(z_mean[:, 0], z_mean[:, 1], z_mean[:, 2], c='blue', alpha=0.7)
        
        ax.set_title("Latent Space Visualization")
        ax.set_xlabel("Z1")
        ax.set_ylabel("Z2")
        ax.set_zlabel("Z3")
        plt.show()
        st.pyplot(fig)  # Display in Streamlit
    else:
        st.warning("Latent dimension is less than 3. 3D visualization requires at least 3 dimensions.")

def run_gan(latent_dim, num_images=10):
    """Runs the GAN and displays generated images."""
    generator, _, _ = create_gan(latent_dim)

    # Generate random vectors for the latent space
    random_latent_vectors = tf.random.normal(shape=(num_images, latent_dim))

    # Generate images from the latent vectors
    generated_images = generator.predict(random_latent_vectors)

    # Visualize the generated images
    plt.figure(figsize=(20, 4))
    for i in range(num_images):
        plt.subplot(1, num_images, i + 1)
        plt.imshow(generated_images[i].reshape(28, 28), cmap="gray")
        plt.axis("off")
    plt.suptitle("Images generated by the GAN")
    st.pyplot(plt)

def train_vae(latent_dim, epochs, batch_size, callbacks_list, loss_function, optimizer_name, learning_rate):
    """Training function with dynamic generation of callbacks and optimizer."""
    # Load MNIST data
    x_train, x_test = load_mnist()
    # Create the optimizer based on parameters
    optimizer = get_optimizer(optimizer_name, learning_rate)
    # Create the VAE model with the custom optimizer
    encoder, decoder, vae = create_vae(latent_dim, optimizer, loss_function)
    print("Training in progress...")

    # Generate callbacks
    callbacks = get_callbacks(callbacks_list)

    # Train the model
    history = vae.fit(
        x_train, x_train, 
        epochs=epochs, 
        batch_size=batch_size, 
        validation_data=(x_test, x_test),
        callbacks=callbacks,
        verbose=1
    )
    
    # Display the loss curve
    plt.figure(figsize=(10, 5))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid()
    st.pyplot(plt)

    # Visualize reconstructions
    num_images = 10
    sample_images = x_test[:num_images]
    z_mean, _, z = encoder.predict(sample_images)
    reconstructions = decoder.predict(z)

    plt.figure(figsize=(20, 4))
    for i in range(num_images):
        plt.subplot(2, num_images, i + 1)
        plt.imshow(sample_images[i].reshape(28, 28), cmap="gray")
        plt.axis("off")
        plt.title("Original")
    
    for i in range(num_images):
        plt.subplot(2, num_images, i + 1 + num_images)
        plt.imshow(reconstructions[i].reshape(28, 28), cmap="gray")
        plt.axis("off")
        plt.title("Reconstructed")
    
    st.pyplot(plt)
    # Add 3D visualization of the latent space
    visualize_latent_space(encoder, x_test)
